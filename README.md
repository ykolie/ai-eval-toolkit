# AI Evaluation Toolkit
A practical toolkit for building and running custom AI evaluations - from basic template matching to model-graded assessments.

# Overview
This toolkit provides battle-tested evaluation methods for measuring LLM performance across different tasks. Whether you're building domain-specific benchmarks or need rigorous model comparisons, these tools help you create reproducible, meaningful evaluations.

# Quick Start
# Install dependencies
pip install -r requirements.txt

# Run a basic evaluation
python examples/basic_eval.py

# Run LLM-as-a-judge evaluation
python examples/llm_judge_eval.py

# Generate evaluation report
python tools/generate_report.py results/

# Evaluation Types

1. Basic Template Evals
* Match: Exact string matching
* Includes: Substring matching
* FuzzyMatch: Approximate string matching
* JSONMatch: Structured data validation

2. LLM-as-a-Judge Evals
* Criteria-based: Multi-dimensional scoring
* Head-to-head: Comparative evaluation
* Chain-of-thought: Reasoning transparency
* Factual consistency: Truth verification

3. Custom Domain Evals
* Security compliance: Policy adherence
* Code quality: Static analysis integration
* Reasoning: Multi-step problem solving
* Safety: Harmful content detection

ğŸ“š Key Concepts
LLM-as-a-Judge
Uses one LLM to evaluate the outputs of another LLM. This approach scales beyond human evaluation while maintaining quality assessment. The judge LLM receives the original prompt, the response to evaluate, and scoring criteria, then provides structured feedback and numerical scores.

ğŸ“Š Example Results
Security Compliance Evaluation Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model           â”‚ Accuracyâ”‚ Precisionâ”‚ Recall  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gpt-4           â”‚   94.2% â”‚    92.1% â”‚   96.3% â”‚
â”‚ claude-3-sonnet â”‚   91.8% â”‚    89.7% â”‚   94.1% â”‚
â”‚ gpt-3.5-turbo   â”‚   87.3% â”‚    85.2% â”‚   89.4% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

